{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bias-Variance\n",
    "\n",
    "In order to minimize the expected test error, we need to select a statistical learning method that simultaneously achieves\n",
    "low variance and low bias.\n",
    "\n",
    "\n",
    "### Variance\n",
    "\n",
    "Variance refers to the amount by which function f would change if we\n",
    "estimated it using a different training data set. Since the training data\n",
    "are used to fit the statistical learning method, different training data sets\n",
    "will result in a different function f. But ideally the estimate for f should not vary\n",
    "too much between training sets. However, if a method has high variance\n",
    "then small changes in the training data can result in large changes in estimated f. In\n",
    "general, more flexible statistical methods have higher variance.\n",
    "\n",
    "\n",
    "### Bias\n",
    "\n",
    "Bias refers to the error that is introduced by approximating\n",
    "a real-life problem, which may be extremely complicated, by a much\n",
    "simpler model. For example, linear regression assumes that there is a linear\n",
    "relationship between Y and X1,X2, . . . , Xp. It is unlikely that any real-life\n",
    "problem truly has such a simple linear relationship, and so performing linear\n",
    "regression will undoubtedly result in some bias in the estimate of f.\n",
    "\n",
    "\n",
    "### Bias-Variance Trade-off\n",
    "\n",
    "In the diagram below, as the complexity increases, the variance increases and bias decreases.\n",
    "This corresponds to the total error (= train_error + test_error) varying as a curve. It first decreases as the complexity grows to a point where the accuracy of the model is optimal. Once it reaches a point where the model starts overfitting the train data, the total error increases again.\n",
    "\n",
    "The point where the total error is minimal is the trade-off point between bias and variance. This will correspond to the most accurate and optimal model for the train and test dataset.\n",
    "\n",
    "<img src=\"../images/complexity-error.png\", style=\"width: 700px;\"> \n",
    "\n",
    "\n",
    "Reference - Introduction to Statistical Learning using R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import plotly.plotly as py\n",
    "import plotly.graph_objs as go\n",
    "from plotly import tools\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Data\n",
    "np.random.seed(0)\n",
    "\n",
    "n_samples = 30\n",
    "degrees = [1, 4, 15]\n",
    "title_msg = ['High Bias', 'Trade-Off', 'High Variance']\n",
    "true_fun = lambda X: np.cos(1.5 * np.pi * X)\n",
    "X = np.sort(np.random.rand(n_samples))\n",
    "y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "titles = []\n",
    "data = [] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for i in range(len(degrees)):\n",
    "    polynomial_features = PolynomialFeatures(degree=degrees[i],\n",
    "                                             include_bias=False)\n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([(\"polynomial_features\", polynomial_features),\n",
    "                         (\"linear_regression\", linear_regression)])\n",
    "    pipeline.fit(X[:, np.newaxis], y)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, X[:, np.newaxis], y,\n",
    "                             scoring=\"neg_mean_squared_error\", cv=10)\n",
    "\n",
    "    X_test = np.linspace(0, 1, 100)\n",
    "    \n",
    "    if(i==1):\n",
    "        leg=True\n",
    "    else:\n",
    "        leg=False\n",
    "    data.append([])\n",
    "    trace1 = go.Scatter(x=X_test, y=pipeline.predict(X_test[:, np.newaxis]), \n",
    "                        name=\"Model\", mode='lines',\n",
    "                        line=dict(color='blue', width=1),\n",
    "                        showlegend=leg)\n",
    "    data[i].append(trace1)\n",
    "    trace2 = go.Scatter(x=X_test, y=true_fun(X_test), \n",
    "                        name=\"True function\", mode='lines',\n",
    "                        line=dict(color='green', width=1),\n",
    "                        showlegend=leg)\n",
    "    data[i].append(trace2)\n",
    "    \n",
    "    trace3 = go.Scatter(x=X, y=y, \n",
    "                        name=\"Samples\", mode='markers',\n",
    "                        marker=dict(color='blue', \n",
    "                                    line=dict(color='black', width=1)),\n",
    "                        showlegend=leg)\n",
    "    data[i].append(trace3)\n",
    "    \n",
    "    titles.append(\"{} <br>MSE = {:.2e}(+/- {:.2e})\".format(title_msg[i],\n",
    "                    -scores.mean(), scores.std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig = tools.make_subplots(rows=1, cols=3,\n",
    "                          subplot_titles=tuple(titles[:3]),\n",
    "                          print_grid=False)\n",
    "\n",
    "for i in range(0, len(data)):\n",
    "    for j in range(0, len(data[i])):\n",
    "        fig.append_trace(data[i][j], 1, i+1)\n",
    "        \n",
    "for i in map(str,range(1, 4)):\n",
    "    y = 'yaxis'+i\n",
    "    x = 'xaxis'+i\n",
    "    fig['layout'][y].update(title='y', showgrid=False,\n",
    "                            showticklabels=False, ticks='')\n",
    "    fig['layout'][x].update(title='x', showgrid=False,\n",
    "                            showticklabels=False, ticks='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Interactive Example\n",
    "In the below visualization, there are 3 sub-plots.\n",
    "\n",
    "#### HIgh Bias\n",
    "The First plot(left most) is the model with High Bias. The Model is simply a linear regression model. This is same as the polynomial of a first degree. The line is perhaps too simplistic for the data points in question. Since the model is too rigid, this has high bias.\n",
    "The blue line represents the linear regression which means high bias.\n",
    "\n",
    "#### High Variance\n",
    "The last plot (right most) is the model with High Variance. The Model is a polynomial regression model. This is assuming a degree of 15. The resulting model function follows the training data points in the closest possible way. But this does not mean the model is the best for this dataset. The reason for that is while this fits the training data well, it is too complex to predict well for unknown data points, meaning that this model is too complex and weighs in the noise data as well. Hence this has high variance.\n",
    "The complex blue curve represents the polynomial regression of degree 15 which has high variance.\n",
    "\n",
    "#### Trade-Off\n",
    "The plot in the center is the model with a reasonable Bias-Variance trade-off. The Model is a polynomial regression model. But this time we have modeled it with a degree of 4 to reasonably follow the training data points in the closest possible way without compromising its efficiency to predict the unknown values. \n",
    "The blue curve represents the polynomial regression of degree 4 (trade-off).\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe id=\"igraph\" scrolling=\"no\" style=\"border:none;\" seamless=\"seamless\" src=\"https://plot.ly/~manirv/24.embed\" height=\"525px\" width=\"100%\"></iframe>"
      ],
      "text/plain": [
       "<plotly.tools.PlotlyDisplay object>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "py.iplot(fig)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
